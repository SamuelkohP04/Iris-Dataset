{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a0caf73",
   "metadata": {},
   "source": [
    "## Iris Dataset - Classifying Flowers\n",
    "\n",
    "### A study of univariate and bivariate analysis between features of flowers\n",
    "You may think that this project primarily focusses on classifying flower data based on its features. However, in this project, it is less focussed on that. We are going to do exploration and data analysis, within features themselves. Particularly, bivariate analysis. \n",
    "\n",
    "In this bivariate analysis, we are using what we have learnt in secondary school, $y = mx + c$, to estimate y values from x values. Here, the y values refer to the dependent variable, or any variable we want. And x values are independent variable. We choose y = petal length, and x = petal width.\n",
    "\n",
    "This project covers the following concepts:\n",
    "- Linear regression with a step-by-step implementation\n",
    "- Polynomial regression\n",
    "- Logistic regression and its implementation\n",
    "- Softmax regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da6e197",
   "metadata": {},
   "source": [
    "### Importing requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecba7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8e938",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style='whitegrid', context='notebook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79324d40",
   "metadata": {},
   "source": [
    "### Loading the Iris Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e080a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris2 = sns.load_dataset('iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40417642",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iris2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3ef2ae",
   "metadata": {},
   "source": [
    "It is time to start with the simplest, but still useful abstraction for our data - a linear regression function. In linear regression, we try to find a linear equation that minimises the distance between the data points and the modeled line. The model function takes the following form:\n",
    "\n",
    "$$y_{i} = \\beta x_{i} + \\alpha + \\epsilon_{i}$$\n",
    "\n",
    "where:\n",
    "- $\\alpha$ is the intercept,\n",
    "- $\\beta$ is the slope of the modeled line\n",
    "- $x$ is the independent variable\n",
    "- $y$ is the dependent variable\n",
    "- $\\epsilon_{i}$ variable is an interesting one. It is the error distance from the sample $i$, to the regressed line."
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "c272ca3b",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c4a5e",
   "metadata": {},
   "source": [
    "The set of all those distances, calculated in the cost function, will give us the values of the unknown parameters that will minimise the cost. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325952d7",
   "metadata": {},
   "source": [
    "### Determination of the cost function\n",
    "We are using method of least squares.\n",
    "\n",
    "$$ J(\\beta_{0}, \\beta_{1}) = \\sum \\limits_{i = 0}^{n} (y_{i} - \\beta_{0} - \\beta_{1} x_{i}) ^{2}$$\n",
    "\n",
    "Least squares function for a linear equation, using the standard variables $\\beta_{0}$ and $\\beta_{1}$, which are used in the next sections.\n",
    "\n",
    "Returns the total differences between all the values ($y_{i}$) and the corresponding point in our regressing line $(\\beta_{0} + \\beta_{1}x_{i})$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b77d80a",
   "metadata": {},
   "source": [
    "## Multiple ways of minimising errors\n",
    "\n",
    "The least squares error function has several ways of getting a solution:\n",
    "\n",
    "- The analytical way\n",
    "- Using the covariance and correlation values\n",
    "- Using gradient descent\n",
    "\n",
    "### Analytical approach\n",
    "\n",
    "Using linear algebra techniques, we first represent the error in a function in a matrix form:\n",
    "\n",
    "$$ J(\\theta) = \\frac{1}{2m} (X \\theta  - y)^{T} (X\\theta - y)$$\n",
    "\n",
    "$J$ is the cost function and has an analytical solution of:\n",
    "\n",
    "$$ \\theta = (X^{T} X^{-1}) X^{T} y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325ec691",
   "metadata": {},
   "source": [
    "### Pros and cons of analytical approach\n",
    "\n",
    "Approach using linear algebra to calculate the minimum error solution is easier. But there are some possible problems with this approach:\n",
    "\n",
    "- First, matrix multiplication and inversion are computationally expensive. Time complexity is $O(n^{2})$ to $O(n^{3})$.\n",
    "- Additionally, and depending on the implementation, this direct approach could have limited accuracy. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1f9098",
   "metadata": {},
   "source": [
    "## Covariance/correlation method\n",
    "\n",
    "Now, it is time to introduce a new way of estimating the coefficient of our regressing line. In the process, we learn other statistical measures, like covariance and correlation. This will also help us when analysing a dataset for the first time and drawing our first conclusions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce790d",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Covariance\n",
    "\n",
    "Covariance is a statistical term, and can be defined as:\n",
    "- A measure of the systematic relationship betwewen a pair of random variables, wherein, a change in one variable is reciprocated by an equivalent change in another variable.\n",
    "\n",
    "Covariance can takae any value between $-\\infty$ to $\\infty$, wherein, a negative value is an indicator of a negative relationship. Whereas, a positive value represents a positive relationship. It also ascertains a linear relationship between the two variables.\n",
    "\n",
    "Therefore, when the value is 0, it indicates no direct linear relationship, and the values tend to form a blob-like distribution.\n",
    "\n",
    "$$cov(x_{i}, y) = \\frac{1}{n-1} \\sum (x - \\bar{x}) (y - \\bar{y})$$\n",
    "\n",
    "where: \n",
    "- $cov$ is the covariance,\n",
    "- $n-1$ is the degrees of freedom."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f4c6006",
   "metadata": {},
   "source": [
    "### Standardisation (scaling method for x values)\n",
    "\n",
    "Center the variable by subtracting the mean and scaling it with the standard deviation of the dataset.\n",
    "\n",
    "$$z = \\frac{x_{i} - \\mu}{\\sigma}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea381bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(X, Y):\n",
    "    xbar = np.mean(X)\n",
    "    ybar = np.mean(Y)\n",
    "    epsilion = 0\n",
    "    for x, y in zip(X, Y):\n",
    "        epsilion = epsilion + (x-xbar) * (y-ybar)\n",
    "    return epsilion / (len(X) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624fe9a2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Result of pre-defined covariance function, cov =\", covariance([1,3,4], [1,0,2]))\n",
    "print(\"Result of numpy covariance function, cov matrix =\", np.cov([1,3,4], [1,0,2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf40030e",
   "metadata": {},
   "source": [
    "The result should be equal to (1,0) and (0,1) of the matrix. In this case, 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af68ade",
   "metadata": {},
   "source": [
    "### Correlation\n",
    "This will be the starting point of our analysis, and we will be extending it towards each axis, with the correlation value.\n",
    "\n",
    "The correlation value determines the degree to which two or more random variables move in tandem. During the study of two variables, if it has been observed that the movement of one variable is concordant, with an equivalent movement in another variable, then, the variables are said to be correlated. \n",
    "\n",
    "The formula is:\n",
    "\n",
    "$$r = \\frac{1}{n-1} * \\frac{\\sum(x_{i} - \\bar{x}_{i}) (y - \\bar{y})} {\\sigma_{x_{i}} \\sigma_{y}}$$\n",
    "\n",
    "When $r$ is a positive value, the more $r$ approaches 1, the more it is directly correlated. Meaning, two variables move in the same direction.\n",
    "When $r$ is a negative value, the more $r$ approaches -1, the more the two variables are in opposite directions. Hence the domain of $r$ is $-1 \\leq r \\leq +1$.\n",
    "\n",
    "Let's break down why the denominator, $\\sigma_{x_{i}} \\sigma_{y_{i}}$, can be considered a measure of variance:\n",
    "\n",
    "- Standard deviation measures the spread of data.\n",
    "- Variance, $\\sigma^{2}$, is the square of standard deviation, and quantifies the average of the squared differences from the mean. \n",
    "- Product of variances quantifies the \"joint variability\" of the two variables It tells us how much X and Y vary together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a44ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation(X, Y):\n",
    "    return (covariance(X, Y) / (np.std(X, ddof=1) * np.std(Y, ddof=1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f322020",
   "metadata": {},
   "source": [
    "### Searching for the slope and intercept with covariance and correlation\n",
    "To approximate a linear equation it is:\n",
    "\n",
    "$$ \\hat{y} = \\hat{\\beta} x + \\hat{\\alpha}$$\n",
    "\n",
    "As we know that this line passes through the average of all the points, we can estimate the intercept, with the only unknown being the estimated slope. It is nothing fancier than just flipping the equation around to get intercept.\n",
    "\n",
    "$$ \\hat{\\alpha} = \\bar{y} - \\hat{\\beta} \\bar{x}$$\n",
    "\n",
    "As slope represents the change of the dependent variable divided by the change in the independent variable. But in this case we are dealing with the variation of the data, rather than absolute differences between coordinates.\n",
    "\n",
    "Slope: a proportion of the variance in the independent variable that covaries with the dependent variable:\n",
    "\n",
    "$$ \\hat{\\beta} = \\frac{Cov(x, y)} {Var(x)} $$\n",
    "\n",
    "As it happens, if our data actually looks like a circular cloud when we plot it, our slope will become zero. This suggests no relationship between both variables.\n",
    "\n",
    "$$ \\hat{\\beta} = \\frac{\\sum\\limits^{n}_{i=1} (x_{i} - \\bar{x}) (y_{i} - \\bar{y})} {\\sum\\limits^{n}_{i=1} (x-\\bar{x})^{2}}$$\n",
    "\n",
    "$$ = r_{xy} \\frac{S_{y}}{S_{x}}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e036dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Result of pre-defined corr function, r =\", correlation([1,1,4,3], [1,0,2,2]))\n",
    "print(\"Result of numpy corr function =\", np.corrcoef([1,1,4,3], [1,0,2,2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ec946",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairplot = sns.pairplot(iris2, size=2.0)\n",
    "pairplot.fig.suptitle('Summary of Variable Pairs, and Univariate Distributions',\n",
    "                      y=1.02)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccbf1f6",
   "metadata": {},
   "source": [
    "Let's select two variables that, from our initial analysis, have the property of being linearly dependent. They are \"petal width\" and \"petal length\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b7b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris2['petal_width']\n",
    "Y = iris2['petal_length']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734a0d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y)\n",
    "plt.xlabel('petal width')\n",
    "plt.ylabel('petal length')\n",
    "plt.title('Distribution of petal length against petal width')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a945eac6",
   "metadata": {},
   "source": [
    "### Creating the prediction function\n",
    "\n",
    "First, let's define the function that will represent the modeled data. This is in the form of linear function, in the form y = beta * x + alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa976c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(alpha, beta, x_i):\n",
    "    return beta * x_i + alpha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a17609",
   "metadata": {},
   "source": [
    "### Defining the error function\n",
    "\n",
    "This shows us the difference between predictions and the expected output during training. As we will explain in depth in the next chapter, we have two main alternatives:\n",
    "\n",
    "- measuring the absolute difference between values (or L1)\n",
    "- or measuring a variant of the square of the difference (or L2).\n",
    "\n",
    "Let's define both versions, including the first formulation inside the second."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10cd0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(alpha, beta, x_i, y_i): #L1\n",
    "    return y_i - predict(alpha, beta, x_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ad34bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_sq_e(alpha, beta, x, y):\n",
    "    total_error = sum((error(alpha, beta, x_i, y_i) ** 2) \n",
    "                      for x_i, y_i in zip(x, y))\n",
    "    \n",
    "    return total_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5a2260",
   "metadata": {},
   "source": [
    "### Correlation fit\n",
    "\n",
    "Now, we will define a function implementing the correlation method, to find the parameters for our regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf6a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlation_fit(x, y):\n",
    "    beta = correlation(x, y) * np.std(y, ddof=1) / np.std(x, ddof=1)\n",
    "    alpha = np.mean(y) - beta * np.mean(x)\n",
    "    \n",
    "    return alpha, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfccab0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha, beta = correlation_fit(X, Y)\n",
    "print(\"Alpha =\", alpha)\n",
    "print(\"Beta =\", beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b14bcb",
   "metadata": {},
   "source": [
    "Let's now graph the regressed line, with the data, in order to intuitively show the appropriateness of the solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffde503",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, Y)\n",
    "xr = np.arange(0, 3.5)\n",
    "plt.plot(xr, (xr * beta) + alpha)\n",
    "\n",
    "plt.xlabel('petal width')\n",
    "plt.ylabel('petal length')\n",
    "plt.title('Best fit line of petal length against petal width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3867aef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3266038",
   "metadata": {},
   "outputs": [],
   "source": [
    "ix = iris2['petal_width']\n",
    "iy = iris2['petal_length']\n",
    "\n",
    "x_plot = np.linspace(0, 2.6, 100)\n",
    "\n",
    "X = ix.values[:, np.newaxis]\n",
    "\n",
    "X_plot = x_plot[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480d424e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.scatter(ix, iy, s=30, marker='o', label='training points')\n",
    "for count, degree in enumerate([3, 6, 20]):\n",
    "    model = make_pipeline(PolynomialFeatures(degree), Ridge())\n",
    "    model.fit(X, iy)\n",
    "    y_plot = model.predict(X_plot)\n",
    "    plt.plot(x_plot, y_plot, label='degree %d' % degree)\n",
    "    \n",
    "plt.legend(loc='upper left')\n",
    "plt.title('Polynomial Coefficients to Describe Data Population')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f05eb1",
   "metadata": {},
   "source": [
    "### Using gradient descent\n",
    "\n",
    "- Linear function variables, $\\beta_{0}$ and $\\beta_{1}$\n",
    "- The number of samples in the sampleset, $m$\n",
    "- The different elements in the sampleset, $x^{(i)}$ and $y^{(i)}$\\\n",
    "\n",
    "Let's start with the error function, $J$. It was defined in previous sections (least squares function). We add $\\frac{1}{2m} at the start of the equation.\n",
    "\n",
    "$$J(\\beta_{0}, \\beta_{1}) = \\frac{1}{2m} \\sum \\limits^{m}_{i=1} ((\\beta_{0} + \\beta_{1} x^{(i)}) - y^{(i)}) ^{2}$$\n",
    "\n",
    "We introduce a new operator, the gradient. \n",
    "Assuming we know how partial derivatives work, it is enough to say that the gradient is a vector containing the mentioned derivatives. \n",
    "\n",
    "$$\\triangledown J(\\beta_{0}, \\beta_{1}) = \\begin{bmatrix} (\\frac{\\delta J}{\\delta \\beta_{0}}) \\\\ (\\frac{\\delta J}{\\delta \\beta_{1}}) \\end{bmatrix} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580f39df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9687ab25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def least_squares(b0, b1, points):\n",
    "    totalError = 0\n",
    "    N = float(len(points))\n",
    "    for x, y in points:\n",
    "        totalError += (y - (b1 * x + b0)) ** 2\n",
    "        \n",
    "    return totalError / 2.*N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce864af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_gradient(b0_current, b1_current, points, learningRate):\n",
    "    b0_gradient = 0\n",
    "    b1_gradient = 0\n",
    "    N = float(len(points))\n",
    "    for x, y in points:\n",
    "        b0_gradient += (1/N) * (y - ((b1_current * x) + b0_current))\n",
    "        b1_gradient += (1/N) * x * (y - ((b1_current * x) + b0_current))\n",
    "    \n",
    "    new_b0 = b0_current + (learningRate * b0_gradient)\n",
    "    new_b1 = b1_current + (learningRate * b1_gradient)\n",
    "    \n",
    "    return [new_b0, new_b1, least_squares(new_b0, new_b1, points)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b3773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gradient_descent(points, starting_b0, starting_b1, learning_rate, num_iterations):\n",
    "    b0 = starting_b0\n",
    "    b1 = starting_b1\n",
    "    slope = []\n",
    "    intersect = []\n",
    "    error = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        b0, b1, e = step_gradient(b0, b1, np.array(points), learning_rate)\n",
    "        slope.append(b1)\n",
    "        intersect.append(b0)\n",
    "        error.append(e)\n",
    "        \n",
    "    return [b0, b1, e, slope, intersect, error]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2200830a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = iris2['petal_width'].tolist()\n",
    "Y = iris2['petal_length'].tolist()\n",
    "\n",
    "points = np.dstack((X, Y))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca94502",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.0001\n",
    "initial_b0 = 0\n",
    "initial_b1 = 0\n",
    "num_iterations = 1000\n",
    "[b0, b1, e, slope, intersect, error] = run_gradient_descent(points, initial_b0, initial_b1, learning_rate, num_iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33b39f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X,Y)\n",
    "xr = np.arange(0, 3.5)\n",
    "plt.plot(xr, (xr*b1) + b0)\n",
    "plt.title('Regression, alpha=0.0001, initial values=(0,0), it=1000')\n",
    "\n",
    "plt.ylabel('Petal length')\n",
    "plt.xlabel('Petal width')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1bfd00",
   "metadata": {},
   "source": [
    "Well, that is poor performance. Let's see what happened with the error during training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f12ddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "xr = np.arange(0, 1000)\n",
    "\n",
    "plt.plot(xr, np.array(error).transpose())\n",
    "plt.title('Error for 1000 iterations')\n",
    "plt.ylabel('Total least squares')\n",
    "plt.xlabel('Number of iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d0ac11",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 #Last one was 0.0001\n",
    "initial_b0 = 0\n",
    "initial_b1 = 0\n",
    "num_iterations = 1000\n",
    "\n",
    "[b0, b1, e, slope, intersect, error] = run_gradient_descent(points, \n",
    "                                                            initial_b0,\n",
    "                                                            initial_b1,\n",
    "                                                            learning_rate,\n",
    "                                                            num_iterations\n",
    "                                                           )\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "xr = np.arange(0, 1000)\n",
    "plt.plot(xr, np.array(error).transpose())\n",
    "plt.title('Error for 1000 iterations, increased step by tenfold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0222319",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X, Y)\n",
    "xr = np.arange(0, 3.5)\n",
    "\n",
    "plt.plot(xr, (xr * b1) + b0)\n",
    "plt.title('Regression, alpha=0.01, initial values=(0,0), it=1000')\n",
    "plt.ylabel('Petal length')\n",
    "plt.xlabel('Petal width')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3609cefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.85 # Last one was 0.0001\n",
    "initial_b0 = 0\n",
    "initial_b1 = 0\n",
    "num_iterations = 1000\n",
    "\n",
    "[b0, b1, e, slope, intersect, error] = run_gradient_descent(points, \n",
    "                                                            initial_b0, \n",
    "                                                            initial_b1, \n",
    "                                                            learning_rate,\n",
    "                                                            num_iterations)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "xr = np.arange(0, 1000)\n",
    "plt.plot(xr, np.array(error).transpose())\n",
    "plt.title('Error for 1000 iterations, big step')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b749e25",
   "metadata": {},
   "source": [
    "This is a bad move. As you can see, the error went to infinity. What happened here?\n",
    "\n",
    "The steps we are taking are so radical that instead of slicing the imaginary bowl we described, we are just jumping around the surface. As the iterations advance, we began to escalate the accumulated errors without control. \n",
    "\n",
    "Another measure that could be taken, is to improve our seed values. The seed value starts from 0. Thisis a very bad idea in general for this technique, especially when you are working with data that is not normalised. There are more reasons to this. So, let's try to initialise the parameter on a pseudo-random location in order to allow the graphics to be the same across the code examples, and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a0f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 # Last one was 0.0001\n",
    "initial_b0 = 0.8 # Random\n",
    "initial_b1 = 1.5 # Random\n",
    "num_iterations = 1000\n",
    "\n",
    "[b0, b1, e, slope, intersect, error] = run_gradient_descent(points, \n",
    "                                                            initial_b0, \n",
    "                                                            initial_b1, \n",
    "                                                            learning_rate,\n",
    "                                                            num_iterations)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "xr = np.arange(0, 1000)\n",
    "plt.plot(xr, np.array(error).transpose())\n",
    "plt.title('Error for 1000 iterations, step 0.001, random initial parameter values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0ae380",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001 # Last one was 0.0001\n",
    "initial_b0 = 0.8 # Random\n",
    "initial_b1 = 1.5 # Random\n",
    "num_iterations = 1000\n",
    "\n",
    "x_mean = np.mean(points[:, 0])\n",
    "y_mean = np.mean(points[:, 1])\n",
    "x_std = np.std(points[:, 0])\n",
    "y_std = np.std(points[:, 1])\n",
    "\n",
    "X_normalised = (points[:,0] - x_mean / x_std)\n",
    "Y_normalised = (points[:,1] - y_mean / y_std)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "plt.scatter(X_normalised, Y_normalised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f679277",
   "metadata": {},
   "outputs": [],
   "source": [
    "points = np.dstack((X_normalised, Y_normalised))[0]\n",
    "learning_rate = 0.001\n",
    "initial_b0 = 0.8\n",
    "initial_b1 = 1.5\n",
    "num_iterations = 1000\n",
    "\n",
    "[b0, b1, e, slope, intersect, error] = run_gradient_descent(points, \n",
    "                                                            initial_b0, \n",
    "                                                            initial_b1, \n",
    "                                                            learning_rate,\n",
    "                                                            num_iterations)\n",
    "\n",
    "plt.figure(figsize=(7,5))\n",
    "xr = np.arange(0, 1000)\n",
    "plt.plot(xr, np.array(error).transpose())\n",
    "plt.title('Error for 1000 iterations, step 0.001, random initial parameter values, normalised initial values')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6ef241",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
